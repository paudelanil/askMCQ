{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook,we will try implementing question-answering based rag on given pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "DOC_PATH = \"../data/raw/QUIC_Protocol.pdf\"\n",
    "CHROMA_PATH = \"../embedings/pdf-qa-system\" \n",
    "\n",
    "# load your pdf doc\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split the doc into smaller chunks i.e. chunk_size=500\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed chunks as vectors\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# get OpenAI Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# embed the chunks as vectors and load them into the database\n",
    "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x799082a9b5e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of a user question (query)\n",
    "query = 'What are the main applications of QUIC?'\n",
    "\n",
    "# retrieve context - top 5 most relevant (closests) chunks to the query vector \n",
    "# (by default Langchain is using cosine distance metric)\n",
    "docs_chroma = db_chroma.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# generate an answer based on given user query and retrieved context information\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and Generate Answer with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crosoft. Popular applications like YouTube, Google Search, and Chrome already use QUIC\\nto deliver faster and more reliable user experiences.\\nWeb Application\\nIn web applications, the QUIC protocol can significantly improve browser loading speeds\\nand reduce page rendering times. This is important to improve user experience. Currently,\\nmany mainstream browsers support the QUIC protocol, such as Google Chrome and Mozilla\\nFirefox.\\nReal-time Audio and Video Communication\\n\\nConclusion\\nQUIC is a game-changing protocol that overcomes the limitations of TCP, offering faster\\nconnections, built-in encryption, and efficient multiplexing. As the backbone of HTTP/3, it\\nimproves web performance, especially for latency-sensitive applications like streaming and\\ngaming. Its ability to handle changes in the mobile network and improve reliability makes it\\nvital for modern internet use. Looking ahead, QUIC is poised to drive greater adoption in\\n\\nOverview\\nThe QUIC protocol, pronounced ”quick”, stands for Quick UDP Internet Connections. It\\nwas developed by Google in 2012 to address latency issues and improve the speed of web\\napplications that use Transmission Control Protocol (TCP). Initially introduced as an ex-\\nperimental protocol, QUIC aimed to optimize HTTP-based communication over the internet\\nby combining the speed of UDP with the reliability and security features typically associated\\n\\ngames. This is important to improve the gaming experience of players.\\nInternet of Things (IoT) Devices\\nWith the rapid development of IoT technology, more and more devices need to connect\\nto the internet. The QUIC protocol is suitable for the connection needs of numerous IoT\\ndevices, reducing network congestion and device energy consumption. This is significant for\\npromoting the development of the IoT industry.\\nConclusion\\n\\nthe QUIC protocol\\nBefore the release of QUIC, TCP was used as the underlying protocol for transferring data in\\nHTTP. However, as the mobile internet continues to develop, there is an increasing demand\\n1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import\n",
    "\n",
    "\n",
    "# you can use a prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Prepare a Multiple Choice Question (MCQ) based on the following context. Provide four options and indicate the correct answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Format the output as follows:\n",
    "1. **Question**: [Insert question here]  \n",
    "   a) [Option 1]  \n",
    "   b) [Option 2]  \n",
    "   c) [Option 3]  \n",
    "   d) [Option 4]  \n",
    "   **Correct Answer**: [Correct option]\n",
    "\"\"\"\n",
    "\n",
    "# load retrieved context and user query in the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "context_text =\"QUIC features\"\n",
    "prompt = prompt_template.format(context=context_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80117/2283080551.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI()\n",
      "/tmp/ipykernel_80117/2283080551.py:5: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response_text = model.predict(prompt)\n"
     ]
    }
   ],
   "source": [
    "## Generate answer with LLM\n",
    "\n",
    "# call LLM model to generate the answer based on the given context and query\n",
    "model = ChatOpenAI()\n",
    "response_text = model.predict(prompt)\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",  # Or any other model\n",
    "    prompt=prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. **Question**: What is one of the key features of QUIC protocol?\\n   a) Low latency\\n   b) High bandwidth\\n   c) Improved security\\n   d) Increased packet loss\\n   **Correct Answer**: a) Low latency'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
