{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration File for Managing API Key as an Environment Variable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up LangSmith tracking: https://smith.langchain.com\n",
    "from langsmith import utils\n",
    "\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define template\n",
    "template = \"What is the capital of {country}?\"\n",
    "\n",
    "# Create a `PromptTemplate` object using the `from_template` method.\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Korea?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the prompt.\n",
    "prompt = prompt_template.format(country=\"Korea\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openai model\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Creation\n",
    "\n",
    "### LCEL (Langchain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt as a `PromptTemplate` object.\n",
    "prompt = PromptTemplate.from_template(\"Please explain {topic} in simple terms.\")\n",
    "\n",
    "\n",
    "# Combine the prompt and model into a chain\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input values are provided in the form of a Python dictionary (key-value pairs).\n",
    "\n",
    "When calling the invoke() function, these input values are passed as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Usability of AI in medical terms refers to how easy and effective it is for healthcare professionals to use artificial intelligence tools in their work. Here are some key points to understand it simply:\\n\\n1. **User-Friendly Design**: AI tools should be designed so that doctors, nurses, and other healthcare workers can easily understand and use them without extensive training.\\n\\n2. **Improving Patient Care**: AI can help in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans, making it easier for healthcare providers to give better care.\\n\\n3. **Efficiency**: AI can automate routine tasks, like data entry or scheduling, allowing healthcare professionals to spend more time with patients and less on administrative work.\\n\\n4. **Decision Support**: AI can analyze large amounts of medical data quickly, helping doctors make informed decisions based on the latest research and patient information.\\n\\n5. **Accessibility**: AI tools can be used in various settings, from hospitals to remote clinics, making advanced medical support available to more people.\\n\\n6. **Feedback and Improvement**: Usability also involves gathering feedback from users to continuously improve AI tools, ensuring they meet the needs of healthcare providers effectively.\\n\\nIn summary, the usability of AI in medicine focuses on making these technologies easy to use, helpful in improving patient care, and efficient in supporting healthcare professionals in their daily tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 269, 'prompt_tokens': 21, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-fc520e00-a9dd-48ab-b7d7-10b365aa721c-0', usage_metadata={'input_tokens': 21, 'output_tokens': 269, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input  = {\"topic\":\"Usability of ai in meidcal term\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.stream(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usability of AI in medical terms refers to how easy and effective it is for healthcare professionals to use artificial intelligence tools in their work. Here are some key points to understand it simply:\n",
      "\n",
      "1. **User-Friendly Design**: AI tools should be designed so that doctors, nurses, and other healthcare workers can easily understand and operate them without extensive training.\n",
      "\n",
      "2. **Improving Patient Care**: AI can help in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans, making it easier for healthcare providers to give better care.\n",
      "\n",
      "3. **Efficiency**: AI can automate routine tasks, like data entry or scheduling, allowing healthcare professionals to spend more time with patients and less on administrative work.\n",
      "\n",
      "4. **Decision Support**: AI can analyze large amounts of medical data quickly, helping doctors make informed decisions based on the latest research and patient information.\n",
      "\n",
      "5. **Accessibility**: AI tools can be used in various settings, from hospitals to remote clinics, making advanced medical support available to more people.\n",
      "\n",
      "6. **Feedback and Improvement**: Usability also involves gathering feedback from users to continuously improve AI tools, ensuring they meet the needs of healthcare providers and patients.\n",
      "\n",
      "In summary, the usability of AI in medicine is about making these advanced tools easy to use, effective in improving patient care, and efficient in supporting healthcare professionals in their daily tasks."
     ]
    }
   ],
   "source": [
    "#streaming output\n",
    "# Streaming Output\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = (\n",
    "    StrOutputParser()\n",
    ")  # Directly returns the model's response as a string without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain  = prompt | model  | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usability of AI in medical terms refers to how easy and effective it is for healthcare professionals to use artificial intelligence tools and systems in their work. Here are some key points to understand it simply:\\n\\n1. **User-Friendly Design**: AI tools should be designed so that doctors, nurses, and other healthcare workers can easily understand and operate them without extensive training.\\n\\n2. **Improving Patient Care**: AI can help in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans, making it easier for healthcare providers to give better care.\\n\\n3. **Efficiency**: AI can automate routine tasks, like data entry or scheduling, allowing healthcare professionals to focus more on patient care rather than administrative work.\\n\\n4. **Decision Support**: AI can analyze large amounts of medical data quickly, helping doctors make informed decisions based on the latest research and patient information.\\n\\n5. **Accessibility**: AI tools can be used in various settings, from hospitals to remote clinics, making advanced medical support available to more people.\\n\\n6. **Feedback and Improvement**: Usability also involves gathering feedback from users to continuously improve AI systems, ensuring they meet the needs of healthcare providers effectively.\\n\\nIn summary, the usability of AI in medicine is about making these advanced tools easy to use, so they can enhance the quality of care and streamline healthcare processes.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Interface\n",
    "\n",
    "---\n",
    "\n",
    "To make it as easy as possible to create custom chains, we've implemented the `Runnable` protocol.\n",
    "\n",
    "The `Runnable` protocol is implemented in most components.\n",
    "\n",
    "It is a standard interface that makes it easy to define custom chains and call them in a standard way. The standard interface includes\n",
    "\n",
    "- `stream`: Streams a chunk of the response.\n",
    "- `invoke`: Invoke a chain on an input.\n",
    "- `batch`: Invoke a chain against a list of inputs.\n",
    "\n",
    "There are also asynchronous methods\n",
    "\n",
    "- `astream`: Stream chunks of the response asynchronously.\n",
    "- `ainvoke`: Invoke a chain asynchronously on an input.\n",
    "- `abatch`: Asynchronously invoke a chain against a list of inputs.\n",
    "- `astream_log`: Streams the final response as well as intermediate steps as they occur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging your trace using langsmit\n",
    "import openai\n",
    "from langsmith import wrappers, traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrappers.wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "pipeline(\"Hello, world!\")\n",
    "# Out:  Hello there! How can I assist you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Instantiate the ChatOpenAI model.\n",
    "model = ChatOpenAI()\n",
    "# Create a prompt template that asks for jokes on a given topic.\n",
    "prompt = PromptTemplate.from_template(\"Describe the {topic} in 3 sentences.\")\n",
    "# Connect the prompt and model to create a conversation chain.\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call a function to batch process a given list of topics\n",
    "chain.batch([{\"topic\": \"ChatGPT\"}, {\"topic\": \"Instagram\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch(\n",
    "    [\n",
    "        {\"topic\": \"ChatGPT\"},\n",
    "        {\"topic\": \"Instagram\"},\n",
    "        {\"topic\": \"multimodal\"},\n",
    "        {\"topic\": \"programming\"},\n",
    "        {\"topic\": \"machineLearning\"},\n",
    "    ],\n",
    "    config={\"max_concurrency\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube is a popular video-sharing platform where users can upload, view, and comment on a wide variety of content. It has become a major source of entertainment, education, and news for millions of people around the world. With millions of videos available on virtually any topic, YouTube has revolutionized the way we consume media."
     ]
    }
   ],
   "source": [
    "# Use an asynchronous stream to process messages in the 'YouTube' topic.\n",
    "async for token in chain.astream({\"topic\": \"YouTube\"}):\n",
    "    # Print the message content. Outputs directly without newlines and empties the buffer.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel\n",
    "\n",
    "Let's take a look at how the LangChain Expression Language supports parallel requests. For example, when you use `RunnableParallel` (often written in dictionary form), you execute each element in parallel.\n",
    "\n",
    "Here's an example of running two tasks in parallel using the `RunnableParallel` class in the `langchain_core.runnables` module.\n",
    "\n",
    "Create two chains (`chain1`, `chain2`) that use the `ChatPromptTemplate.from_template` method to get the capital and area for a given `country`.\n",
    "\n",
    "These chains are connected via the `model` and pipe (`|`) operators, respectively. Finally, we use the `RunnableParallel` class to combine these two chains with the keys `capital` and `area` to create a `combined` object that can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Create a chain that asks for the capital of {country}.\n",
    "chain1 = (\n",
    "    PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain that asks for the area of {country}.\n",
    "chain2 = (\n",
    "    PromptTemplate.from_template(\"What is the area of {country}?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel execution chain that generates the above two chains in parallel.\n",
    "combined = RunnableParallel(capital=chain1, area=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  capital: PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?')\n",
       "           | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e96b1111600>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e96b1110d90>, root_client=<openai.OpenAI object at 0x7e96b2b2d120>, root_async_client=<openai.AsyncOpenAI object at 0x7e96b1113d00>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser(),\n",
       "  area: PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the area of {country}?')\n",
       "        | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e96b1111600>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e96b1110d90>, root_client=<openai.OpenAI object at 0x7e96b2b2d120>, root_async_client=<openai.AsyncOpenAI object at 0x7e96b1113d00>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "        | StrOutputParser()\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's `Runnable` objects provide a modular and flexible approach to designing workflows by enabling the chaining, parallel execution, and transformation of data. These utilities allow for efficient handling of structured inputs and outputs, with minimal code overhead.\n",
    "\n",
    "Key Components is:\n",
    "\n",
    "- **`RunnableLambda`**: A lightweight utility that enables the application of custom logic through lambda functions, ideal for dynamic and quick data transformations.\n",
    "- **`RunnablePassthrough`**: Designed to pass input data unchanged or augment it with additional attributes when paired with the `.assign()` method.\n",
    "- **`itemgetter`**: A Python `operator` module utility for efficiently extracting specific keys or indices from structured data such as dictionaries or tuples.\n",
    "\n",
    "These tools can be combined to build powerful workflows, such as:\n",
    "\n",
    "- Extracting and processing specific data elements using `itemgetter`.\n",
    "- Performing custom transformations with `RunnableLambda`.\n",
    "- Creating end-to-end data pipelines with `Runnable` chains.\n",
    "\n",
    "By leveraging these components, users can design scalable and reusable pipelines for machine learning and data processing workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunablePassthrough\n",
    "\n",
    "`RunnablePassthrough` is a utility designed to streamline data processing workflows by either passing input data unchanged or enhancing it with additional attributes. Its flexibility makes it a valuable tool for handling data in pipelines where minimal transformation or selective augmentation is required.\n",
    "\n",
    "1. **Simple Data Forwarding**\n",
    "\n",
    "- Suitable for scenarios where no transformation is required, such as logging raw data or passing it to downstream systems.\n",
    "\n",
    "2. **Dynamic Data Augmentation**\n",
    "\n",
    "- Enables the addition of metadata or context to input data for use in machine learning pipelines or analytics systems.\n",
    "a\n",
    "---\n",
    "- `RunnablePassthrough` can either pass the input unchanged or append additional keys to it.\n",
    "- When `RunnablePassthrough()` is called on its own, it simply takes the input and passes it as is.\n",
    "- When called using `RunnablePassthrough.assign(...)`, it takes the input and adds additional arguments provided to the assign function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the prompt and llm\n",
    "prompt = PromptTemplate.from_template(\"What is 10 times {num}?\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create the chaina\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "#create chain using RunnablePassthrough\n",
    "\n",
    "runnable_chain = {\"num\":RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  num: RunnablePassthrough()\n",
       "}\n",
       "| PromptTemplate(input_variables=['num'], input_types={}, partial_variables={}, template='What is 10 times {num}?')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e96b118ae30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e96b118bf10>, root_client=<openai.OpenAI object at 0x7e96b11894e0>, root_async_client=<openai.AsyncOpenAI object at 0x7e96b118ae00>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 times 2 is equal to 20.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_chain.invoke(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign new key pair using .assign()\n",
    "\n",
    "(RunnablePassthrough.assign(num=lambda x: x[\"num\"] * 3)).invoke({\"num\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2, 'num': 2}\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_chain_assign.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Function that returns the length of a sentence.\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "# Function that returns the product of the lengths of two sentences.\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "# Function that uses _multiple_length_function to return the product of the lengths of two sentences.\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {a} + {b}?\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"word1\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"word1\"), \"text2\": itemgetter(\"word2\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  a: RunnableLambda(itemgetter('word1'))\n",
       "     | RunnableLambda(length_function),\n",
       "  b: {\n",
       "       text1: RunnableLambda(itemgetter('word1')),\n",
       "       text2: RunnableLambda(itemgetter('word2'))\n",
       "     }\n",
       "     | RunnableLambda(multiple_length_function)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['a', 'b'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['a', 'b'], input_types={}, partial_variables={}, template='What is {a} + {b}?'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e96a49a5600>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e96a49a7430>, root_client=<openai.OpenAI object at 0x7e96b0fb77f0>, root_async_client=<openai.AsyncOpenAI object at 0x7e96a49a55d0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': 'United States of America'}, template='What are the capitals of {country1} and {country2}, respectively?')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define template\n",
    "template = \"What are the capitals of {country1} and {country2}, respectively?\"\n",
    "\n",
    "# Create a prompt template with `PromptTemplate` object\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country1\"],\n",
    "    partial_variables={\n",
    "        \"country2\": \"United States of America\"  # Pass `partial_variables` in dictionary form\n",
    "    },\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the capitals of Nepal and United States of America, respectively?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(country1=\"Nepal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': 'India'}, template='What are the capitals of {country1} and {country2}, respectively?')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_partial = prompt.partial(country2=\"India\")\n",
    "prompt_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_partial | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Nepal is Kathmandu, and the capital of India is New Delhi.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"nepal\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using partial_variables\n",
    "\n",
    "Using `partial_variables` , you can partially apply functions.  This is particularly useful when there are **common variables** to be shared.\n",
    "\n",
    "Common examples are **date or time**.\n",
    "\n",
    "Suppose you want to specify the current date in your prompt, hardcoding the date into the prompt or passing it along with other input variables may not be practical. In this case, using a function that returns the current date to modify the prompt partially is much more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Print the current date\n",
    "datetime.now().strftime(\"%B %d\")\n",
    "\n",
    "\n",
    "# Define function to return the current date\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%B %d\")\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Today's date is {today}. Please list {n} celebrities whose birthday is today. Please specify their date of birth.\",\n",
    "    input_variables=[\"n\"],\n",
    "    partial_variables={\n",
    "        \"today\": get_today  # Pass `partial_variables` in dictionary form\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is January 20. Please list 1 celebrities whose birthday is today. Please specify their date of birth.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prompt template from YAML file\n",
    "\n",
    "You can manage prompt templates in seperate yaml files and load using `load_prompt` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for PromptTemplate\ninput_variables\n  Field required [type=missing, input_value={'prompt_template': {'tem...ables': ['user_input']}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ntemplate\n  Field required [type=missing, input_value={'prompt_template': {'tem...ables': ['user_input']}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_prompt\n\u001b[0;32m----> 3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mload_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompts/fruit_color.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m prompt\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/langchain_core/prompts/loading.py:163\u001b[0m, in \u001b[0;36mload_prompt\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    157\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading from the deprecated github-based Hub is no longer supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use the new LangChain Hub at https://smith.langchain.com/hub \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_prompt_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/langchain_core/prompts/loading.py:183\u001b[0m, in \u001b[0;36m_load_prompt_from_file\u001b[0;34m(file, encoding)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Load the prompt from the config now.\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_prompt_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/langchain_core/prompts/loading.py:41\u001b[0m, in \u001b[0;36mload_prompt_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m     40\u001b[0m prompt_loader \u001b[38;5;241m=\u001b[39m type_to_loader_dict[config_type]\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprompt_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/langchain_core/prompts/loading.py:138\u001b[0m, in \u001b[0;36m_load_prompt\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    131\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading templates with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format is no longer supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince it can lead to arbitrary code execution. Please migrate to using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m template format, which does not suffer from this issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RAG/lib/python3.10/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for PromptTemplate\ninput_variables\n  Field required [type=missing, input_value={'prompt_template': {'tem...ables': ['user_input']}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ntemplate\n  Field required [type=missing, input_value={'prompt_template': {'tem...ables': ['user_input']}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"prompts/fruit_color.yaml\", encoding=\"utf-8\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` can be used to include a conversation history as a prompt.\n",
    "\n",
    "Messages are structured as tuples in the format (`role` , `message` ) and are created as a list.\n",
    "\n",
    "**role**\n",
    "- `\"system\"` : A system setup message, typically used for global settings-related prompts.\n",
    "- `\"human\"` : A user input message.\n",
    "- `\"ai\"` : An AI response message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}?'), additional_kwargs={})])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a friendly AI assistant. Your name is Teddy.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Nice to meet you!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"You are a friendly AI assistant. Your name is {name}.\"),\n",
    "        (\"human\", \"Nice to meet you!\"),\n",
    "        (\"ai\", \"Hello! How can I assist you?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create chat messages\n",
    "messages = chat_template.format_messages(name=\"Teddy\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Anil. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 50, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a823204c-0717-441f-aba6-2b7e033521c8-0', usage_metadata={'input_tokens': 50, 'output_tokens': 14, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_template | llm\n",
    "chain.invoke({\"name\":\"Anil\",\"user_input\":\"What is your name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's Few-Shot Prompting provides a robust framework for guiding language models to generate high-quality outputs by supplying carefully selected examples. This technique minimizes the need for extensive model fine-tuning while ensuring precise, context-aware results across diverse use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Few-Shot Prompt Templates: Define the structure and format of prompts by embedding illustrative examples, guiding the model to produce consistent outputs.\n",
    "\n",
    "Example Selection Strategies: Dynamically select the most relevant examples for a given query, enhancing the model's contextual understanding and response accuracy.\n",
    "\n",
    "Chroma Vector Store: A powerful utility for storing and retrieving examples based on semantic similarity, enabling scalable and efficient prompt construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of the United States of America is Washington, D.C.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 17, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-b4691eba-3a93-43d9-9be3-4b31843f889e-0' usage_metadata={'input_tokens': 17, 'output_tokens': 15, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,  # Creativity\n",
    "    model_name=\"gpt-4o-mini\",  # Use a valid model name\n",
    ")\n",
    "\n",
    "# User query\n",
    "question = \"What is the capital of United States of America?\"\n",
    "\n",
    "# Query the model\n",
    "response = llm.invoke(question)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Define examples for the few-shot prompt\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Steve Jobs or Einstein?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: At what age did Steve Jobs die?\n",
    "Intermediate Answer: Steve Jobs died at the age of 56.\n",
    "Additional Question: At what age did Einstein die?\n",
    "Intermediate Answer: Einstein died at the age of 76.\n",
    "The final answer is: Einstein\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of Naver born?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is the founder of Naver?\n",
    "Intermediate Answer: Naver was founded by Lee Hae-jin.\n",
    "Additional Question: When was Lee Hae-jin born?\n",
    "Intermediate Answer: Lee Hae-jin was born on June 22, 1967.\n",
    "The final answer is: June 22, 1967\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the reigning king when Yulgok Yi's mother was born?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is Yulgok Yi's mother?\n",
    "Intermediate Answer: Yulgok Yi's mother is Shin Saimdang.\n",
    "Additional Question: When was Shin Saimdang born?\n",
    "Intermediate Answer: Shin Saimdang was born in 1504.\n",
    "Additional Question: Who was the king of Joseon in 1504?\n",
    "Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.\n",
    "The final answer is: Yeonsangun\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are the directors of Oldboy and Parasite from the same country?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is the director of Oldboy?\n",
    "Intermediate Answer: The director of Oldboy is Park Chan-wook.\n",
    "Additional Question: Which country is Park Chan-wook from?\n",
    "Intermediate Answer: Park Chan-wook is from South Korea.\n",
    "Additional Question: Who is the director of Parasite?\n",
    "Intermediate Answer: The director of Parasite is Bong Joon-ho.\n",
    "Additional Question: Which country is Bong Joon-ho from?\n",
    "Intermediate Answer: Bong Joon-ho is from South Korea.\n",
    "The final answer is: Yes\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Who lived longer, Steve Jobs or Einstein?\n",
      "Answer:\n",
      "Does this question require additional questions: Yes.\n",
      "Additional Question: At what age did Steve Jobs die?\n",
      "Intermediate Answer: Steve Jobs died at the age of 56.\n",
      "Additional Question: At what age did Einstein die?\n",
      "Intermediate Answer: Einstein died at the age of 76.\n",
      "The final answer is: Einstein\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an example prompt template\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Question:\\n{question}\\nAnswer:\\n{answer}\"\n",
    ")\n",
    "\n",
    "# Print the first formatted example\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Who lived longer, Steve Jobs or Einstein?\n",
      "Answer:\n",
      "Does this question require additional questions: Yes.\n",
      "Additional Question: At what age did Steve Jobs die?\n",
      "Intermediate Answer: Steve Jobs died at the age of 56.\n",
      "Additional Question: At what age did Einstein die?\n",
      "Intermediate Answer: Einstein died at the age of 76.\n",
      "The final answer is: Einstein\n",
      "\n",
      "\n",
      "Question:\n",
      "When was the founder of Naver born?\n",
      "Answer:\n",
      "Does this question require additional questions: Yes.\n",
      "Additional Question: Who is the founder of Naver?\n",
      "Intermediate Answer: Naver was founded by Lee Hae-jin.\n",
      "Additional Question: When was Lee Hae-jin born?\n",
      "Intermediate Answer: Lee Hae-jin was born on June 22, 1967.\n",
      "The final answer is: June 22, 1967\n",
      "\n",
      "\n",
      "Question:\n",
      "Who was the reigning king when Yulgok Yi's mother was born?\n",
      "Answer:\n",
      "Does this question require additional questions: Yes.\n",
      "Additional Question: Who is Yulgok Yi's mother?\n",
      "Intermediate Answer: Yulgok Yi's mother is Shin Saimdang.\n",
      "Additional Question: When was Shin Saimdang born?\n",
      "Intermediate Answer: Shin Saimdang was born in 1504.\n",
      "Additional Question: Who was the king of Joseon in 1504?\n",
      "Intermediate Answer: The king of Joseon in 1504 was Yeonsangun.\n",
      "The final answer is: Yeonsangun\n",
      "\n",
      "\n",
      "Question:\n",
      "Are the directors of Oldboy and Parasite from the same country?\n",
      "Answer:\n",
      "Does this question require additional questions: Yes.\n",
      "Additional Question: Who is the director of Oldboy?\n",
      "Intermediate Answer: The director of Oldboy is Park Chan-wook.\n",
      "Additional Question: Which country is Park Chan-wook from?\n",
      "Intermediate Answer: Park Chan-wook is from South Korea.\n",
      "Additional Question: Who is the director of Parasite?\n",
      "Intermediate Answer: The director of Parasite is Bong Joon-ho.\n",
      "Additional Question: Which country is Bong Joon-ho from?\n",
      "Intermediate Answer: Bong Joon-ho is from South Korea.\n",
      "The final answer is: Yes\n",
      "\n",
      "\n",
      "Question:\n",
      "How old was Bill Gates when Google was founded?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FewShotPromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# Example question\n",
    "question = \"How old was Bill Gates when Google was founded?\"\n",
    "\n",
    "# Generate the final prompt\n",
    "final_prompt = few_shot_prompt.format(question=question)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does this question require additional questions: Yes.  \\nAdditional Question: When was Google founded?  \\nIntermediate Answer: Google was founded on September 4, 1998.  \\nAdditional Question: What year was Bill Gates born?  \\nIntermediate Answer: Bill Gates was born on October 28, 1955.  \\nAdditional Question: How old was Bill Gates on September 4, 1998?  \\nIntermediate Answer: Bill Gates was 42 years old when Google was founded.  \\nThe final answer is: 42 years old.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain =  few_shot_prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"question\": \"How old was Bill Gates when Google was founded?\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Exmple Selection with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings.base import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "chroma = Chroma(persist_directory=\"example_selector\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Steve Jobs or Einstein?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: At what age did Steve Jobs die?\n",
    "Intermediate Answer: Steve Jobs died at the age of 56.\n",
    "Additional Question: At what age did Einstein die?\n",
    "Intermediate Answer: Einstein died at the age of 76.\n",
    "The final answer is: Einstein\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of Google born?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is the founder of Google?\n",
    "Intermediate Answer: Google was founded by Larry Page and Sergey Brin.\n",
    "Additional Question: When was Larry Page born?\n",
    "Intermediate Answer: Larry Page was born on March 26, 1973.\n",
    "Additional Question: When was Sergey Brin born?\n",
    "Intermediate Answer: Sergey Brin was born on August 21, 1973.\n",
    "The final answer is: Larry Page was born on March 26, 1973, and Sergey Brin was born on August 21, 1973.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the President when Donald Trump's mother was born?\",  #  \n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is Donald Trump's mother?\n",
    "Intermediate Answer: Donald Trump's mother is Mary Anne MacLeod Trump.\n",
    "Additional Question: When was Mary Anne MacLeod Trump born?\n",
    "Intermediate Answer: Mary Anne MacLeod Trump was born on May 10, 1912.\n",
    "Additional Question: Who was the U.S. President in 1912?\n",
    "Intermediate Answer: William Howard Taft was President in 1912.\n",
    "The final answer is: William Howard Taft\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are the directors of Oldboy and Parasite from the same country?\",\n",
    "        \"answer\": \"\"\"Does this question require additional questions: Yes.\n",
    "Additional Question: Who is the director of Oldboy?\n",
    "Intermediate Answer: The director of Oldboy is Park Chan-wook.\n",
    "Additional Question: Which country is Park Chan-wook from?\n",
    "Intermediate Answer: Park Chan-wook is from South Korea.\n",
    "Additional Question: Who is the director of Parasite?\n",
    "Intermediate Answer: The director of Parasite is Bong Joon-ho.\n",
    "Additional Question: Which country is Bong Joon-ho from?\n",
    "Intermediate Answer: Bong Joon-ho is from South Korea.\n",
    "The final answer is: Yes\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Who lived longer, Steve Jobs or Einstein?',\n",
       "  'answer': 'Does this question require additional questions: Yes.\\nAdditional Question: At what age did Steve Jobs die?\\nIntermediate Answer: Steve Jobs died at the age of 56.\\nAdditional Question: At what age did Einstein die?\\nIntermediate Answer: Einstein died at the age of 76.\\nThe final answer is: Einstein\\n'},\n",
       " {'question': 'When was the founder of Google born?',\n",
       "  'answer': 'Does this question require additional questions: Yes.\\nAdditional Question: Who is the founder of Google?\\nIntermediate Answer: Google was founded by Larry Page and Sergey Brin.\\nAdditional Question: When was Larry Page born?\\nIntermediate Answer: Larry Page was born on March 26, 1973.\\nAdditional Question: When was Sergey Brin born?\\nIntermediate Answer: Sergey Brin was born on August 21, 1973.\\nThe final answer is: Larry Page was born on March 26, 1973, and Sergey Brin was born on August 21, 1973.\\n'},\n",
       " {'question': \"Who was the President when Donald Trump's mother was born?\",\n",
       "  'answer': \"Does this question require additional questions: Yes.\\nAdditional Question: Who is Donald Trump's mother?\\nIntermediate Answer: Donald Trump's mother is Mary Anne MacLeod Trump.\\nAdditional Question: When was Mary Anne MacLeod Trump born?\\nIntermediate Answer: Mary Anne MacLeod Trump was born on May 10, 1912.\\nAdditional Question: Who was the U.S. President in 1912?\\nIntermediate Answer: William Howard Taft was President in 1912.\\nThe final answer is: William Howard Taft\\n\"},\n",
       " {'question': 'Are the directors of Oldboy and Parasite from the same country?',\n",
       "  'answer': 'Does this question require additional questions: Yes.\\nAdditional Question: Who is the director of Oldboy?\\nIntermediate Answer: The director of Oldboy is Park Chan-wook.\\nAdditional Question: Which country is Park Chan-wook from?\\nIntermediate Answer: Park Chan-wook is from South Korea.\\nAdditional Question: Who is the director of Parasite?\\nIntermediate Answer: The director of Parasite is Bong Joon-ho.\\nAdditional Question: Which country is Bong Joon-ho from?\\nIntermediate Answer: Bong Joon-ho is from South Korea.\\nThe final answer is: Yes\\n'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [example[\"question\"] for example in examples]\n",
    "metadatas = [example for example in examples]\n",
    "metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add examples to the vector store\n",
    "texts = [example[\"question\"] for example in examples]\n",
    "metadatas = [example for example in examples]\n",
    "chroma.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "# Set up Example Selector\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=chroma,  # Only vectorstore is needed\n",
    "    k=1  # Number of examples to select\n",
    ")\n",
    "\n",
    "# Define Few-Shot Prompt Template\n",
    "example_prompt_template = PromptTemplate.from_template(\n",
    "    \"Question:\\n{question}\\nAnswer:\\n{answer}\\n\"\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt_template,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"question\": \"Is Nepal larger than America?\"}\n",
    "formatted_prompt = prompt.format(**query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question:\\nWho lived longer, Steve Jobs or Einstein?\\nAnswer:\\nDoes this question require additional questions: Yes.\\nAdditional Question: At what age did Steve Jobs die?\\nIntermediate Answer: Steve Jobs died at the age of 56.\\nAdditional Question: At what age did Einstein die?\\nIntermediate Answer: Einstein died at the age of 76.\\nThe final answer is: Einstein\\n\\n\\n\\nQuestion:\\nIs Nepal larger than America?\\nAnswer:'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does this question require additional questions: Yes.  \\nAdditional Question: What are the total land areas of Nepal and America?  \\nIntermediate Answer: Nepal has a land area of approximately 147,516 square kilometers, while the United States has a land area of about 9.8 million square kilometers.  \\nThe final answer is: No, Nepal is not larger than America.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_title = \"llm-response-evaluation\"\n",
    "\n",
    "evaluation_prompt = \"\"\"Evaluate the LLM's response based on the following criteria:\n",
    "\n",
    "INPUT:\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "LLM Response: {answer}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Accuracy (0-10)\n",
    "- Perfect (10): Completely accurate, perfectly aligned with context\n",
    "- Good (7-9): Minor inaccuracies\n",
    "- Fair (4-6): Some significant inaccuracies\n",
    "- Poor (0-3): Major inaccuracies or misalignment\n",
    "\n",
    "2. Completeness (0-10)\n",
    "- Perfect (10): Comprehensive coverage of all relevant points\n",
    "- Good (7-9): Covers most important points\n",
    "- Fair (4-6): Missing several key points\n",
    "- Poor (0-3): Critically incomplete\n",
    "\n",
    "3. Context Relevance (0-10)\n",
    "- Perfect (10): Optimal use of context\n",
    "- Good (7-9): Good use with minor omissions\n",
    "- Fair (4-6): Partial use of relevant context\n",
    "- Poor (0-3): Poor context utilization\n",
    "\n",
    "4. Clarity (0-10)\n",
    "- Perfect (10): Exceptionally clear and well-structured\n",
    "- Good (7-9): Clear with minor issues\n",
    "- Fair (4-6): Somewhat unclear\n",
    "- Poor (0-3): Confusing or poorly structured\n",
    "\n",
    "SCORING METHOD:\n",
    "1. Calculate individual scores\n",
    "2. Compute weighted average:\n",
    "   - Accuracy: 40%\n",
    "   - Completeness: 25%\n",
    "   - Context Relevance: 25%\n",
    "   - Clarity: 10%\n",
    "3. Normalize to 0-1 scale\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{\n",
    "    \"individual_scores\": {\n",
    "        \"accuracy\": float,\n",
    "        \"completeness\": float,\n",
    "        \"context_relevance\": float,\n",
    "        \"clarity\": float\n",
    "    },\n",
    "    \"weighted_score\": float,\n",
    "    \"normalized_score\": float,\n",
    "    \"evaluation_notes\": string\n",
    "}\n",
    "\n",
    "Return ONLY the normalized_score as a decimal between 0 and 1.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(evaluation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "As a medical professional, analyze clinical cases with:\n",
    "\n",
    "INPUT:\n",
    "- Patient Information: {patient_data}\n",
    "- Clinical Notes: {clinical_notes}\n",
    "\n",
    "PROVIDE:\n",
    "1. Clinical Assessment\n",
    "2. Diagnostic Process\n",
    "3. Treatment Plan\n",
    "4. Risk Assessment\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{\n",
    "    \"clinical_summary\": str,\n",
    "    \"differential_diagnosis\": list,\n",
    "    \"treatment_plan\": dict,\n",
    "    \"risk_assessment\": dict\n",
    "}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
